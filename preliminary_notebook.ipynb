{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "class LearnablePrompt(nn.Module):\n",
    "    \"\"\"\n",
    "    A trainable prompt representation for the Instruct-Pix2Pix model.\n",
    "    Uses numerical parameters rather than specific tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, template=\"Make the image {}\", \n",
    "                 num_params=10, embedding_dim=768):\n",
    "        super().__init__()\n",
    "        self.template = template\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Freeze the text encoder - we're only learning the parameters\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Create trainable continuous parameters\n",
    "        self.num_params = num_params\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_params = nn.Parameter(torch.randn(num_params) * 0.02)\n",
    "        \n",
    "        # Linear projection to map from our parameters to the text embedding space\n",
    "        self.projection = nn.Linear(num_params, embedding_dim)\n",
    "        \n",
    "        # Initialize projection with small weights\n",
    "        nn.init.xavier_uniform_(self.projection.weight, gain=0.0001)\n",
    "        nn.init.zeros_(self.projection.bias)\n",
    "        \n",
    "    def get_params_snapshot(self):\n",
    "        \"\"\"Get the current parameter values\"\"\"\n",
    "        return self.context_params.detach().cpu().numpy()\n",
    "        \n",
    "    def forward(self, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate text prompt using the current parameters\n",
    "        \n",
    "        Returns:\n",
    "            prompt: Text prompt for the diffusion model\n",
    "            text_embeddings: Text embeddings for advanced use cases\n",
    "        \"\"\"\n",
    "        # Convert parameters to text embeddings through projection\n",
    "        projected_params = self.projection(self.context_params)\n",
    "        \n",
    "        # For the text prompt, we'll use a fixed template with a placeholder\n",
    "        # The actual impact comes from the embeddings we inject into the diffusion process\n",
    "        base_prompt = self.template.format(\"with optimized parameters\")\n",
    "        \n",
    "        return base_prompt, projected_params\n",
    "    \n",
    "    def get_text_embeddings(self, prompt):\n",
    "        \"\"\"\n",
    "        Get text embeddings for a prompt, useful for direct manipulation\n",
    "        \"\"\"\n",
    "        text_inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_embeddings = self.text_encoder(\n",
    "                text_inputs.input_ids.to(self.context_params.device)\n",
    "            )[0]\n",
    "            \n",
    "        return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from models.learnable_prompt import LearnablePrompt\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
    "\n",
    "class AdversarialSetup:\n",
    "    \"\"\"\n",
    "    Complete pipeline for adversarial attack on DINOv2 using Instruct-Pix2Pix.\n",
    "    Uses numerical parameters for optimization rather than specific tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.dinov2_model = None\n",
    "        self.dinov2_processor = None\n",
    "        self.pix2pix_model = None\n",
    "        self.prompt_model = None\n",
    "        \n",
    "    def load_models(self):\n",
    "        \"\"\"Load the required models from HuggingFace\"\"\"\n",
    "        print(\"Loading DINOv2 model...\")\n",
    "        self.dinov2_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small\")\n",
    "        self.dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-small\").to(self.device)\n",
    "        self.dinov2_model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        print(\"Loading Instruct-Pix2Pix model...\")\n",
    "        self.pix2pix_model = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "            \"timbrooks/instruct-pix2pix\", \n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Disable safety checker for this experiment\n",
    "        self.pix2pix_model.safety_checker = None\n",
    "        \n",
    "        # Freeze all Instruct-Pix2Pix parameters\n",
    "        for param in self.pix2pix_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        print(\"Models loaded successfully!\")\n",
    "    \n",
    "    def initialize_prompt_model(self, template=\"Transform the image to {}\", \n",
    "                              num_params=10, embedding_dim=768):\n",
    "        \"\"\"\n",
    "        Initialize the learnable prompt model with numerical parameters\n",
    "        \n",
    "        Args:\n",
    "            template: Template string for the prompt\n",
    "            num_params: Number of trainable parameters\n",
    "            embedding_dim: Dimension of the embedding space\n",
    "        \"\"\"\n",
    "        self.prompt_model = LearnablePrompt(\n",
    "            template=template,\n",
    "            num_params=num_params,\n",
    "            embedding_dim=embedding_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        return self.prompt_model\n",
    "    \n",
    "    def get_dinov2_representation(self, image):\n",
    "        \"\"\"\n",
    "        Get DINOv2 representation for an image\n",
    "        image: PIL Image or preprocessed tensor\n",
    "        \"\"\"\n",
    "        if isinstance(image, Image.Image):\n",
    "            # Preprocess the image\n",
    "            inputs = self.dinov2_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.dinov2_model(**inputs)\n",
    "            return outputs.last_hidden_state[:, 0]  # CLS token embedding\n",
    "        else:\n",
    "            # Assume image is already a preprocessed tensor\n",
    "            with torch.no_grad():\n",
    "                outputs = self.dinov2_model(pixel_values=image)\n",
    "            return outputs.last_hidden_state[:, 0]\n",
    "            \n",
    "    def apply_pix2pix(self, image, prompt_text, prompt_embedding=None, num_steps=20):\n",
    "        \"\"\"\n",
    "        Apply Instruct-Pix2Pix to generate a modified image\n",
    "        \n",
    "        Args:\n",
    "            image: Input image\n",
    "            prompt_text: Text prompt\n",
    "            prompt_embedding: Optional embedding to influence generation\n",
    "            num_steps: Number of diffusion steps\n",
    "        \"\"\"\n",
    "        # For this example, we'll use the standard pipeline interface\n",
    "        # In a more advanced implementation, you could modify the pipeline internals\n",
    "        # to directly inject the embeddings\n",
    "        output = self.pix2pix_model(\n",
    "            prompt_text,\n",
    "            image=image, \n",
    "            num_inference_steps=num_steps, \n",
    "            image_guidance_scale=1.5,\n",
    "            guidance_scale=7.0\n",
    "        )\n",
    "        return output.images[0]\n",
    "        \n",
    "    def representation_distance(self, rep1, rep2, metric=\"cosine\"):\n",
    "        \"\"\"\n",
    "        Calculate distance between two representations\n",
    "        We want to maximize this distance (minimize similarity)\n",
    "        \"\"\"\n",
    "        if metric == \"cosine\":\n",
    "            # Normalize vectors\n",
    "            rep1_norm = rep1 / rep1.norm(dim=1, keepdim=True)\n",
    "            rep2_norm = rep2 / rep2.norm(dim=1, keepdim=True)\n",
    "            # Compute cosine distance (1 - similarity)\n",
    "            similarity = (rep1_norm * rep2_norm).sum(dim=1)\n",
    "            distance = 1 - similarity\n",
    "        elif metric == \"l2\":\n",
    "            # L2 distance\n",
    "            distance = torch.norm(rep1 - rep2, dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {metric}\")\n",
    "            \n",
    "        return distance.mean()\n",
    "    \n",
    "    def full_pipeline(self, image, get_gradients=False):\n",
    "        \"\"\"\n",
    "        Run the full adversarial pipeline with numerical parameters:\n",
    "        1. Generate prompt from learnable parameters\n",
    "        2. Apply Pix2Pix to modify image\n",
    "        3. Get DINOv2 representations of original and modified images\n",
    "        4. Calculate representation distance\n",
    "        \"\"\"\n",
    "        # Step 1: Generate prompt from learnable parameters\n",
    "        prompt_text, prompt_embedding = self.prompt_model()\n",
    "        \n",
    "        # Step 2: Apply Pix2Pix to modify image\n",
    "        modified_image = self.apply_pix2pix(\n",
    "            image, \n",
    "            prompt_text=prompt_text, \n",
    "            prompt_embedding=prompt_embedding\n",
    "        )\n",
    "        \n",
    "        # Step 3: Get DINOv2 representations\n",
    "        # For original image (no gradients needed)\n",
    "        with torch.no_grad():\n",
    "            original_inputs = self.dinov2_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "            original_outputs = self.dinov2_model(**original_inputs)\n",
    "            original_rep = original_outputs.last_hidden_state[:, 0]  # CLS token \n",
    "        \n",
    "        # For modified image (with gradients if needed)\n",
    "        if get_gradients:\n",
    "            # Convert PIL to tensor with preprocessing\n",
    "            modified_inputs = self.dinov2_processor(images=modified_image, return_tensors=\"pt\").to(self.device)\n",
    "            modified_outputs = self.dinov2_model(**modified_inputs)\n",
    "            modified_rep = modified_outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                modified_inputs = self.dinov2_processor(images=modified_image, return_tensors=\"pt\").to(self.device)\n",
    "                modified_outputs = self.dinov2_model(**modified_inputs)\n",
    "                modified_rep = modified_outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        \n",
    "        # Step 4: Calculate representation distance\n",
    "        distance = self.representation_distance(original_rep, modified_rep)\n",
    "        \n",
    "        return modified_image, original_rep, modified_rep, distance, prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from models.adversarial_setup import AdversarialSetup\n",
    "from PIL import Image\n",
    "\n",
    "def train_adversarial_prompt(adv_setup, original_image, num_iterations=50, \n",
    "                           lr=0.01):\n",
    "    \"\"\"\n",
    "    Train the prompt model with numerical parameters to maximize representation distance\n",
    "    \n",
    "    Args:\n",
    "        adv_setup: Initialized AdversarialSetup object\n",
    "        original_image: PIL Image to start from\n",
    "        num_iterations: Number of training iterations\n",
    "        lr: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        history: Dictionary containing training history\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(adv_setup.prompt_model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {\n",
    "        'distance': [],\n",
    "        'prompts': [],\n",
    "        'images': [],\n",
    "        'parameter_values': []\n",
    "    }\n",
    "    \n",
    "    # Main training loop\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Apply full pipeline with gradients\n",
    "        modified_image, original_rep, modified_rep, distance, prompt = adv_setup.full_pipeline(\n",
    "            original_image, get_gradients=True\n",
    "        )\n",
    "        \n",
    "        # Loss is negative distance (we want to maximize distance)\n",
    "        loss = -distance\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log progress\n",
    "        history['distance'].append(distance.item())\n",
    "        history['prompts'].append(prompt)\n",
    "        \n",
    "        # Get parameter values for analysis\n",
    "        param_values = adv_setup.prompt_model.get_params_snapshot()\n",
    "        history['parameter_values'].append(param_values)\n",
    "        \n",
    "        # Save image every 10 iterations\n",
    "        if i % 10 == 0 or i == num_iterations - 1:\n",
    "            history['images'].append(modified_image)\n",
    "            \n",
    "        # Print progress\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Iteration {i}, Distance: {distance.item():.4f}, Prompt: {prompt}\")\n",
    "            print(f\"Parameter values range: [{param_values.min():.4f}, {param_values.max():.4f}]\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_training_results(original_image, history):\n",
    "    \"\"\"Visualize the training progress and results with numerical parameters\"\"\"\n",
    "    # Plot distance over iterations\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['distance'])\n",
    "    plt.title('Representation Distance Over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Display original and modified images\n",
    "    num_images = len(history['images'])\n",
    "    fig, axes = plt.subplots(1, num_images + 1, figsize=(5*(num_images+1), 5))\n",
    "    \n",
    "    # Show original image\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show modified images at different iterations\n",
    "    for i, img in enumerate(history['images']):\n",
    "        iter_num = i * 10 if i < num_images - 1 else len(history['distance']) - 1\n",
    "        axes[i+1].imshow(img)\n",
    "        axes[i+1].set_title(f\"Iteration {iter_num}\")\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot parameter evolution\n",
    "    param_values = np.array(history['parameter_values'])\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot each parameter's value over time\n",
    "    for i in range(param_values.shape[1]):\n",
    "        plt.plot(param_values[:, i], label=f'Param {i}')\n",
    "    \n",
    "    plt.title('Parameter Values Over Training')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Show heatmap of parameter values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(param_values.T, aspect='auto', cmap='viridis')\n",
    "    plt.colorbar(label='Parameter Value')\n",
    "    plt.title('Parameter Evolution Heatmap')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Parameter Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.adversarial_setup import AdversarialSetup\n",
    "from train import train_adversarial_prompt\n",
    "from utils.visualization import visualize_training_results\n",
    "from PIL import Image\n",
    "\n",
    "def main():\n",
    "    # Load image\n",
    "    original_image = Image.open(\"path/to/your/image.jpg\").convert(\"RGB\")\n",
    "    \n",
    "    # Setup with numerical parameters\n",
    "    adv_setup = AdversarialSetup()\n",
    "    adv_setup.load_models()\n",
    "    \n",
    "    # Initialize with 10 trainable numerical parameters\n",
    "    adv_setup.initialize_prompt_model(\n",
    "        template=\"Transform the image to {}\",\n",
    "        num_params=10,\n",
    "        embedding_dim=768\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = train_adversarial_prompt(\n",
    "        adv_setup, \n",
    "        original_image,\n",
    "        num_iterations=50,\n",
    "        lr=0.01\n",
    "    )\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_training_results(original_image, history)\n",
    "    \n",
    "    # Save final adversarial image\n",
    "    history['images'][-1].save(\"adversarial_result.jpg\")\n",
    "    print(f\"Final adversarial image saved to adversarial_result.jpg\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
