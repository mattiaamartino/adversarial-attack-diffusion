{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "\n",
    "import clip\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
    "\n",
    "from attacker_network import AttackerNetwork\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='dogs_data/Images/', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino = AutoModel.from_pretrained(\"facebook/dinov2-small\")\n",
    "dino.to(device)\n",
    "dino.eval()\n",
    "for p in dino.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "clip_model = clip.load(\"ViT-L/14\", device=device)[0]\n",
    "clip_model.eval()\n",
    "\n",
    "\n",
    "pix2pix = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "    \"timbrooks/instruct-pix2pix\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    safety_checker=None,\n",
    ").to(device)\n",
    "\n",
    "for component in [pix2pix.vae, pix2pix.unet, pix2pix.text_encoder]:\n",
    "    for param in component.parameters():\n",
    "        param.requires_grad = False\n",
    "    component.eval()\n",
    "\n",
    "pix2pix.unet.enable_gradient_checkpointing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(model):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for images, label in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = model(images).last_hidden_state[:, 0].cpu().numpy()\n",
    "        features.append(feature)\n",
    "        labels.extend(label.cpu().numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    unique_classes = np.unique(labels)\n",
    "    n_classes = len(unique_classes)\n",
    "\n",
    "    centroids = np.zeros((n_classes, features.shape[1]))\n",
    "    for i, c in enumerate(unique_classes):\n",
    "        class_indices = np.where(labels == c)[0]\n",
    "        centroids[i] = np.mean(features[class_indices], axis=0)\n",
    "\n",
    "    os.makedirs(\"variables\", exist_ok=True)\n",
    "    np.save('variables/centroids.npy', centroids)\n",
    "    np.save('variables/features.npy', features)\n",
    "    np.save('variables/labels.npy', labels)\n",
    "\n",
    "    return centroids, features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids, features, labels = compute_centroids(dino)\n",
    "\n",
    "centroids = np.load('variables/centroids.npy')\n",
    "features = np.load('variables/features.npy')\n",
    "labels = np.load('variables/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "unique_classes = np.unique(labels)\n",
    "n_classes = len(unique_classes)\n",
    "\n",
    "class_id_to_name = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "class_names = [class_id_to_name[cls] for cls in unique_classes]\n",
    "\n",
    "random_classes = np.random.choice(unique_classes, size=2, replace=False)\n",
    "starting_class = random_classes[0]\n",
    "starting_class_name = class_id_to_name[starting_class].split('-')[-1]\n",
    "target_class = random_classes[1]\n",
    "target_class_name = class_id_to_name[target_class].split('-')[-1]\n",
    "\n",
    "classes = {starting_class: starting_class_name + \" (starting)\", target_class: target_class_name + \" (target)\"}\n",
    "\n",
    "print(f\"Starting class: {starting_class_name} ({starting_class})\")\n",
    "print(f\"Target class: {target_class_name} ({target_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "subset_size = 100\n",
    "\n",
    "starting_features = []\n",
    "target_features = []\n",
    "for c in random_classes:\n",
    "    class_indices = np.where(labels == c)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, size=subset_size, replace=False)\n",
    "    if c == starting_class:\n",
    "        starting_features.append(features[sampled_indices])\n",
    "    else:\n",
    "        target_features.append(features[sampled_indices])\n",
    "\n",
    "starting_features = np.concatenate(starting_features, axis=0)\n",
    "target_features = np.concatenate(target_features, axis=0)\n",
    "\n",
    "starting_labels = np.full((starting_features.shape[0],), starting_class)\n",
    "target_labels = np.full((target_features.shape[0],), target_class)\n",
    "\n",
    "plotting_features = np.concatenate([starting_features, target_features], axis=0)\n",
    "plotting_labels = np.concatenate([starting_labels, target_labels], axis=0)\n",
    "\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=0)\n",
    "embedding = umap_reducer.fit_transform(plotting_features)\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(random_classes)))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plot features for each selected class with a distinct color\n",
    "for i, cls in enumerate(random_classes):\n",
    "    cls_mask = (plotting_labels == cls)\n",
    "    plt.scatter(\n",
    "        embedding[cls_mask, 0],\n",
    "        embedding[cls_mask, 1],\n",
    "        s=50,\n",
    "        color=colors[i],\n",
    "        label=classes[cls]\n",
    "    )\n",
    "\n",
    "plt.title(\"UMAP Projection of Image Features for Starting Class and Target Class\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_centroid = centroids[starting_class]\n",
    "target_centroid = centroids[target_class]\n",
    "starting_centroid = starting_centroid / np.linalg.norm(starting_centroid)\n",
    "target_centroid = target_centroid / np.linalg.norm(target_centroid)\n",
    "\n",
    "# Compute the distance between the centroids\n",
    "distance = cosine_distances(starting_centroid.reshape(1, -1), target_centroid.reshape(1, -1))[0][0]\n",
    "print(f\"Distance between centroids: {distance:.2f}\")\n",
    "\n",
    "distances = cosine_distances(centroids)\n",
    "average_distance = distances.mean()\n",
    "print(f\"Average distance between centroids: {average_distance:.2f}\")\n",
    "\n",
    "min_distance = distances[~np.eye(distances.shape[0], dtype=bool)].min()\n",
    "print(f\"Minimum distance between centroids: {min_distance:.2f}\")\n",
    "\n",
    "max_distance = distances[~np.eye(distances.shape[0], dtype=bool)].max()\n",
    "print(f\"Maximum distance between centroids: {max_distance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_indices = [i for i, (_, label) in enumerate(dataset) if label == starting_class]\n",
    "# target_indices = np.save('variables/target_indices.npy', target_indices)\n",
    "target_indices = np.load('variables/target_indices.npy')\n",
    "breed_subset = Subset(dataset, target_indices)\n",
    "\n",
    "pil_to_tensor = transforms.ToTensor() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = breed_subset[0][0]\n",
    "img = img.unsqueeze(0).to(device)\n",
    "feat = dino(img).last_hidden_state[:, 0].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances(starting_centroid.reshape(1, -1), feat.reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "attacker = AttackerNetwork(\n",
    "    device=device,\n",
    "    pix2pix_model=pix2pix,\n",
    "    dinov2_model=dino,\n",
    "    clip_model=clip_model,\n",
    ")\n",
    "\n",
    "n_epochs = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(attacker.parameters(), lr=1e-3)\n",
    "\n",
    "target_centroid = torch.tensor(target_centroid).to(device).unsqueeze(0)\n",
    "target_centroid = F.normalize(target_centroid, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = breed_subset[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.to(device)\n",
    "\n",
    "output, _, _ = attacker(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = F.normalize(output, dim=1)\n",
    "\n",
    "cosine_sim = F.cosine_similarity(output.squeeze(0), target_centroid, dim=1)\n",
    "cosine_dist = 1 - cosine_sim\n",
    "\n",
    "loss = cosine_dist.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for image, _ in tqdm(breed_subset, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "        image = image.to(device)\n",
    "\n",
    "        output, _, _ = attacker(image)\n",
    "        output_norm = F.normalize(output, dim=1)\n",
    "\n",
    "\n",
    "        cosine_sim = F.cosine_similarity(output_norm, target_centroid, dim=1)\n",
    "        cosine_dist = 1 - cosine_sim\n",
    "\n",
    "        loss = cosine_dist.mean()\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
