{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "\n",
    "import clip\n",
    "from InstructDiffusionWrapper import InstructDiffusion\n",
    "\n",
    "from attacker_network import AttackerNetwork\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         #std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='dogs_data/Images/', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ldm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m clip_model \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-L/14\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m clip_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 9\u001b[0m pix2pix \u001b[38;5;241m=\u001b[39m \u001b[43mInstructDiffusion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInstructDiffusion/configs/instruct_diffusion.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInstructDiffusion/checkpoints/v1-5-pruned-emaonly-adaption-task-humanalign.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m pix2pix\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m pix2pix\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/adversarial-attack-diffusion/InstructDiffusionWrapper.py:34\u001b[0m, in \u001b[0;36mInstructDiffusion.__init__\u001b[0;34m(self, config_path, ckpt_path, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m config \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(config_path)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Instantiate model skeleton\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/adversarial-attack-diffusion/InstructDiffusionWrapper.py:118\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, ckpt, vae_ckpt, verbose)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model_from_config\u001b[39m(config, ckpt, vae_ckpt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 118\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m     pl_sd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/adversarial-attack-diffusion/InstructDiffusion/stable_diffusion/ldm/util.py:85\u001b[0m, in \u001b[0;36minstantiate_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected key `target` to instantiate.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_obj_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mdict\u001b[39m()))\n",
      "File \u001b[0;32m~/adversarial-attack-diffusion/InstructDiffusion/stable_diffusion/ldm/util.py:93\u001b[0m, in \u001b[0;36mget_obj_from_str\u001b[0;34m(string, reload)\u001b[0m\n\u001b[1;32m     91\u001b[0m     module_imp \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(module)\n\u001b[1;32m     92\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mreload(module_imp)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m, \u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.15/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ldm'"
     ]
    }
   ],
   "source": [
    "dino = AutoModel.from_pretrained(\"facebook/dinov2-small\")\n",
    "dino.to(device)\n",
    "dino.eval()\n",
    "\n",
    "clip_model = clip.load(\"ViT-L/14\", device=device)[0]\n",
    "clip_model.eval()\n",
    "\n",
    "\n",
    "pix2pix = InstructDiffusion(\n",
    "    \"InstructDiffusion/configs/instruct_diffusion.yaml\",\n",
    "    \"InstructDiffusion/checkpoints/v1-5-pruned-emaonly-adaption-task-humanalign.ckpt\"\n",
    "    )\n",
    "\n",
    "pix2pix.to(device)\n",
    "pix2pix.eval()\n",
    "\n",
    "dino_preprocessor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = torch.randn(1, 4, 64, 64, requires_grad=True).to(device)\n",
    "output_test = pix2pix.model.decode_first_stage(z_test)\n",
    "output_test.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(model):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for images, label in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = model(images).last_hidden_state[:, 0].cpu().numpy()\n",
    "        features.append(feature)\n",
    "        labels.extend(label.cpu().numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    unique_classes = np.unique(labels)\n",
    "    n_classes = len(unique_classes)\n",
    "\n",
    "    centroids = np.zeros((n_classes, features.shape[1]))\n",
    "    for i, c in enumerate(unique_classes):\n",
    "        class_indices = np.where(labels == c)[0]\n",
    "        centroids[i] = np.mean(features[class_indices], axis=0)\n",
    "\n",
    "    os.makedirs(\"variables\", exist_ok=True)\n",
    "    np.save('variables/centroids.npy', centroids)\n",
    "    np.save('variables/features.npy', features)\n",
    "    np.save('variables/labels.npy', labels)\n",
    "\n",
    "    return centroids, features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids, features, labels = compute_centroids(dino)\n",
    "\n",
    "centroids = np.load('variables/centroids.npy')\n",
    "features = np.load('variables/features.npy')\n",
    "labels = np.load('variables/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "unique_classes = np.unique(labels)\n",
    "n_classes = len(unique_classes)\n",
    "\n",
    "class_id_to_name = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "class_names = [class_id_to_name[cls] for cls in unique_classes]\n",
    "\n",
    "random_classes = np.random.choice(unique_classes, size=2, replace=False)\n",
    "starting_class = random_classes[0]\n",
    "starting_class_name = class_id_to_name[starting_class].split('-')[-1]\n",
    "target_class = random_classes[1]\n",
    "target_class_name = class_id_to_name[target_class].split('-')[-1]\n",
    "\n",
    "classes = {starting_class: starting_class_name + \" (starting)\", target_class: target_class_name + \" (target)\"}\n",
    "\n",
    "print(f\"Starting class: {starting_class_name} ({starting_class})\")\n",
    "print(f\"Target class: {target_class_name} ({target_class})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "subset_size = 100\n",
    "\n",
    "starting_features = []\n",
    "target_features = []\n",
    "for c in random_classes:\n",
    "    class_indices = np.where(labels == c)[0]\n",
    "    sampled_indices = np.random.choice(class_indices, size=subset_size, replace=False)\n",
    "    if c == starting_class:\n",
    "        starting_features.append(features[sampled_indices])\n",
    "    else:\n",
    "        target_features.append(features[sampled_indices])\n",
    "\n",
    "starting_features = np.concatenate(starting_features, axis=0)\n",
    "target_features = np.concatenate(target_features, axis=0)\n",
    "\n",
    "starting_labels = np.full((starting_features.shape[0],), starting_class)\n",
    "target_labels = np.full((target_features.shape[0],), target_class)\n",
    "\n",
    "plotting_features = np.concatenate([starting_features, target_features], axis=0)\n",
    "plotting_labels = np.concatenate([starting_labels, target_labels], axis=0)\n",
    "\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=0)\n",
    "embedding = umap_reducer.fit_transform(plotting_features)\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(random_classes)))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Plot features for each selected class with a distinct color\n",
    "for i, cls in enumerate(random_classes):\n",
    "    cls_mask = (plotting_labels == cls)\n",
    "    plt.scatter(\n",
    "        embedding[cls_mask, 0],\n",
    "        embedding[cls_mask, 1],\n",
    "        s=50,\n",
    "        color=colors[i],\n",
    "        label=classes[cls]\n",
    "    )\n",
    "\n",
    "plt.title(\"UMAP Projection of Image Features for Starting Class and Target Class\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_centroid = centroids[starting_class]\n",
    "target_centroid = centroids[target_class]\n",
    "starting_centroid = starting_centroid / np.linalg.norm(starting_centroid)\n",
    "target_centroid = target_centroid / np.linalg.norm(target_centroid)\n",
    "\n",
    "# Compute the distance between the centroids\n",
    "distance = cosine_distances(starting_centroid.reshape(1, -1), target_centroid.reshape(1, -1))[0][0]\n",
    "print(f\"Cosine distance between centroids: {distance:.2f}\")\n",
    "\n",
    "distances = cosine_distances(centroids)\n",
    "average_distance = distances.mean()\n",
    "print(f\"Average cosine distance between centroids: {average_distance:.2f}\")\n",
    "\n",
    "min_distance = distances[~np.eye(distances.shape[0], dtype=bool)].min()\n",
    "print(f\"Minimum cosine distance between centroids: {min_distance:.2f}\")\n",
    "\n",
    "max_distance = distances[~np.eye(distances.shape[0], dtype=bool)].max()\n",
    "print(f\"Maximum cosine distance between centroids: {max_distance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_indices = [i for i, (_, label) in enumerate(dataset) if label == starting_class]\n",
    "# target_indices = np.save('variables/target_indices.npy', target_indices)\n",
    "target_indices = np.load('variables/target_indices.npy')\n",
    "breed_subset = Subset(dataset, target_indices)\n",
    "\n",
    "pil_to_tensor = transforms.ToTensor() \n",
    "tensor_to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "attacker = AttackerNetwork(\n",
    "    device=device,\n",
    "    pix2pix_model=pix2pix,\n",
    "    dinov2_model=dino,\n",
    "    clip_model=clip_model,\n",
    "    \n",
    ")\n",
    "attacker.train()\n",
    "\n",
    "n_epochs = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(attacker.parameters(), lr=1e-3)\n",
    "\n",
    "target_centroid = torch.tensor(target_centroid).to(device).unsqueeze(0)\n",
    "target_centroid = F.normalize(target_centroid, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = breed_subset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "output, modified_image = attacker(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = F.normalize(output, dim=1)\n",
    "\n",
    "cosine_sim = F.cosine_similarity(output.squeeze(0), target_centroid, dim=1)\n",
    "cosine_dist = 1 - cosine_sim\n",
    "\n",
    "loss = cosine_dist.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker.positive_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for image, _ in tqdm(breed_subset, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
    "        image = image.to(device)\n",
    "\n",
    "        output, _, _ = attacker(image)\n",
    "        output_norm = F.normalize(output, dim=1)\n",
    "\n",
    "\n",
    "        cosine_sim = F.cosine_similarity(output_norm, target_centroid, dim=1)\n",
    "        cosine_dist = 1 - cosine_sim\n",
    "\n",
    "        loss = cosine_dist.mean()\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
