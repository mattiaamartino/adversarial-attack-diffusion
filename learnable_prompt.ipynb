{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there is the complete and fully functional implementation of the learnable prompt, either it is positive or negative, can pass thorugh this module and we have a learnable context. We will use this as the embedding input of the instruct-pix2pix model \n",
    "\n",
    "\n",
    "Next steps:\n",
    "\n",
    "- Implement the full attack class constructing the right connection between the learnable prompt in the instruct-pix2pix model and the instruct-pix2pix in the dino-small model.\n",
    "\n",
    "- Test if the learnable prompt with 0 context and a random prompt injected in the model vs. the model with the same random prompt lead to similar output (cannot achieve same as the sampling is not deterministic) or we can directly check their embeddings.\n",
    "\n",
    "- Devise which strategy to use to train the learnable prompt (one prompt for each image? same prompt for different images of the same class? ) and build the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "\n",
    "class LearnablePrompt(nn.Module):\n",
    "    \"\"\"\n",
    "    A trainable prompt representation for the Instruct-Pix2Pix model.\n",
    "    We want to learn a set of parameters that can be used to generate\n",
    "    a text prompt for the diffusion model. The parameters are optimized\n",
    "    during training to attack the representation space of the diffusion model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, template=\"Make the image: \", \n",
    "                 ctx_len=10, clip_model = \"ViT-L/14\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.template = template\n",
    "        self.clip_model = clip.load(clip_model, device=device)[0]\n",
    "        self.tokenizer = clip.tokenize\n",
    "        \n",
    "        # Freezing the text encoder - we're only learning the parameters\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Create trainable continuous parameters\n",
    "        self.ctx_len = ctx_len\n",
    "        self.max_length = 77\n",
    "        self.embedding_dim = self.clip_model.token_embedding.weight.shape[1]\n",
    "\n",
    "        # Initialize context parameters\n",
    "        self.context = nn.Parameter(torch.randn(1, ctx_len, self.embedding_dim, device=self.device) * 0.02)\n",
    "\n",
    "        # Pre-compute template tokens\n",
    "        self.template_tokens = self.tokenizer(self.template).to(self.device)\n",
    "\n",
    "        # Get the position of the EOS token in the template\n",
    "        mask = self.template_tokens[0] == 49407\n",
    "        eos_idx = int(torch.nonzero(mask).squeeze())\n",
    "\n",
    "        self.prefix_embed = self.clip_model.token_embedding(self.template_tokens)[:, :eos_idx].detach() #[SOS] + template\n",
    "        self.eos_embed = self.clip_model.token_embedding(self.template_tokens)[:, eos_idx:].detach()    #[EOS]\n",
    "\n",
    "        # Calculate total sequence length\n",
    "        self.total_length = eos_idx + ctx_len + 1  # SOS + template + ctx + EOS\n",
    "\n",
    "        #This is just a mask for getting the position of the eos token\n",
    "        self.tokenized_prompt = torch.zeros(1, self.max_length, device = self.device)\n",
    "        self.tokenized_prompt[: , self.total_length - 1] = 1\n",
    "\n",
    "        if self.total_length > self.max_length:\n",
    "            print(\n",
    "                f\"Warning: Total sequence length ({self.total_length}) exceeds maximum length ({self.max_length}). \"\n",
    "                \"Sequence will be truncated.\"\n",
    "            )\n",
    "        \n",
    "    def get_params_snapshot(self):\n",
    "        \"\"\"Get the current parameter values\"\"\"\n",
    "        return self.context.detach().cpu().numpy()\n",
    "    \n",
    "    def _encode_prompt(self, full_embeddings):\n",
    "        full_embeddings = full_embeddings + self.clip_model.positional_embedding\n",
    "        full_embeddings = full_embeddings.permute(1, 0, 2) # [batch_size, seq_len, dim] -> [seq_len, batch_size, dim]\n",
    "        full_embeddings = self.clip_model.transformer(full_embeddings)\n",
    "        full_embeddings = full_embeddings.permute(1, 0, 2) # [seq_len, batch_size, dim] -> [batch_size, seq_len, dim]\n",
    "        full_embeddings = self.clip_model.ln_final(full_embeddings) #.type(self.dtype)\n",
    "\n",
    "        return full_embeddings[torch.arange(full_embeddings.shape[0], device=full_embeddings.device), self.tokenized_prompt.argmax(dim=-1)] @ self.clip_model.text_projection\n",
    "    \n",
    "    def _build_full_embeddings(self):\n",
    "        print(\"Here!!\")\n",
    "        # Combine all embeddings: [SOS] + template + ctx + [EOS]\n",
    "        full_embeddings = torch.cat([\n",
    "            self.prefix_embed,\n",
    "            self.context,\n",
    "            self.eos_embed,\n",
    "        ], dim=1)[:, :77]\n",
    "        \n",
    "        print(f\"Full embeddings shape: {full_embeddings.shape}\")\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Returns the text embeddings with structure: [SOS] + template + ctx + [EOS]\n",
    "        \n",
    "        Returns:\n",
    "            display_prompt (str): Text representation of the prompt (for display)\n",
    "            text_embeddings (torch.Tensor): Complete text embeddings\n",
    "        \"\"\"\n",
    "        # Combine all embeddings: [SOS] + template + ctx + [EOS]\n",
    "        full_embeddings = torch.cat([\n",
    "            self.prefix_embed,\n",
    "            self.context,\n",
    "            self.eos_embed,\n",
    "        ], dim=1)[:, :77]\n",
    "\n",
    "        # Encode the prompt\n",
    "        output = self._encode_prompt(full_embeddings)   \n",
    "        \n",
    "        # Create display prompt\n",
    "        #display_prompt = f\"{self.template} [+ {self.ctx_len} context tokens]\"\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = LearnablePrompt(\n",
    "    device=\"cpu\", \n",
    "    template=\"Make the image: \", \n",
    "    ctx_len=10, \n",
    "    clip_model = \"ViT-L/14\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
    "import torch\n",
    "model = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
    "    \"timbrooks/instruct-pix2pix\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ").to(\"mps\" if torch.mps.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
